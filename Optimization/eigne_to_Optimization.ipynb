{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. What are the basic elements of optimization? Give some examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typicall, An optimization problem is defined by four parts: \n",
    "a set of decision variables, \n",
    "an objective function, \n",
    "bounds on the decision variables, and \n",
    "constraints.\n",
    "- decision variables : They are set of variables which we are looking to change to find the optimal solution. A solution is a set of values assigned to these decision variables.\n",
    "- objective function : It is a function $f(\\vec{X})$ of decision variables which gives the single value while determining the quality of different solutions. Depending upon how we frame the optimization problem, we try to minimize or maximize it. \n",
    "- bounds on the decision variables : Bounds defines what values (upper and lowe bounds ) and what kind of  decision variables (boolean, integer) are allowed to take. \n",
    "- constraints : defining restrictions on decision variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Why is optimization important in machine learning? Give some examples. \n",
    "Optimization is one the most important aspect in any machine learning technique. In machine learning, optimization starts with defining some sorts of loss/cost function and trying to minimize it using some kind of optimization technique. One of the area where optimizing is used in machine learning is in optimizing the  parameter of machine learning model. Parameter is a configuration variable that is internal to the model and whose value can be estimated from the given data.\n",
    " ##### Examples of model parameters include:\n",
    "- weights in an artificial neural network.\n",
    "- coefficients in a linear regression or logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is a loss function /cost function? Give examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, cost function is the a way of evaluating how well your algorithm models our dataset. If our predictions are totally off, our cost function will output a higher number. If they’re pretty good, it’ll output a lower number.\n",
    "Cross-entropy and mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a gradient operator?  **Trick question :What are its possible eigenfunctions and eigenvalues?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient operator is a vector differential operator (operator defined as a function of the differentiation operator), when applied to scaler field, it denotes the gradient of field. In other word, It is a vector that, given a scalar field, points to the directive of the steepest change in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How is Hessian related to the gradient, eigenvalues, optimization and convergence? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know gradient is rate of change of some function in each direction or derivative of a multi-variable function.  And Hessian matrix is square-matrix of second order partial derivatives of multivariable scaler valued function.  In other words, it is the rate of change of the slope. And the  rate of change of the slope corresponds to how “curved” is function. Hence,Hessian matrix represents the curvature of function. More precisely, the eigenvalues of the Hessian represents the curvature of the loss function is in the direction of the corresponding eigenvector. Larger the eigen values, faster the convergence. SO, directions with large eigenvalues need smaller step-sizes during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the relation between quadratic form, Hessian, precision matrix, Mahalanobis distance and optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of both Hessian and precision matrix, eigen value and eigen vector provides information about data.\n",
    "In precision matrix, the eigenvectors stay the same, but the eigenvalues are the reciprocals of the eigenvalues of the covariance. That means the biggest eigenvalues of the covariance will be the smallest eigenvalues of the precision. The largest eigenvector of the covariance matrix always points into the direction of the largest variance of the data, and the magnitude of this vector equals the corresponding eigenvalue.\n",
    "Similarly, in Hessian matrix the eigenvalues of the represents the curvature of the loss function is in the direction of the corresponding eigenvector.\n",
    "And Mahalonobis distance is the distance between a point and a distribution which is effectively a multivariate equivalent of the Euclidean distance.\n",
    "Mahalonobis distance is given by $$ D^2 = (x-m)^t. C^{-1} (x-m)$$\n",
    "- $D^2$        is the square of the Mahalanobis distance. \n",
    " - x          is the vector of the observation (row in a dataset), \n",
    " - m          is the vector of mean values of independent variables (mean of each column), \n",
    " - $C^{-1}$     is the inverse covariance matrix of independent variables. \n",
    " \n",
    "So,what we see is that dividing by covariance in above formulam is essentially a multivariate equivalent of the regular standardization. So  it addresses both the problems of scale as well as the correlation of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is momentum and what problem is it solving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment is a paramter in gradient descent algorithm which is used to correct the condition called “ill-conditioning” Hessian i.e when the ratio between the largest and smallest eigenvalues is large. As discussed earlier, the eigenvalues of the Hessian represents the curvature of the loss function is in the direction of the corresponding eigenvector. Larger the eigen values, faster the convergence. So, when we have a larger value of step size in the direction of larger curvature, there is chance of over shoting the miniumum and if there is smaller step size in direction of smaller curvature, it takes a lot of time in converging. So, what moment does it reduces the step size in directions of larger curvature and increses it in directions of smaller curvature. So, it finds the ideal step size or learning rate depending upon the curvature of function. This helps to accelerate gradients vectors in the right directions, thus leading to faster and accurate converging.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
