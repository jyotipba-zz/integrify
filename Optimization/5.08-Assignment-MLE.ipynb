{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is a likelihood function? Also add a formula. \n",
    "Likelihood function is a function that measures how possible is each value of the parameter given some  data.\n",
    "Let $X_1$,$X_2$,$X_3$...$X_n$ have joint density function f($X_1$,$X_2$,$X_3$...$X_n|\\theta$)\n",
    "where  $\\theta$ be a unknown parameter(s) of model. Given $X_1=x_1$,$X_2=x_2$,$X_3=x_3$...$X_n=x_n$ is observed , the function of $\\theta$ defined by $$L(\\theta)= L(\\theta|x_1,x_2,...x_n)=f(x_1,x_2,...x_n|\\theta)$$ is likelihood function.\n",
    "\n",
    "- The likelihood function is not a probability density function.\n",
    "- For iid data, likelihood function is $$L(\\theta)= \\prod_{i=1}^n f(X_i;\\theta)$$\n",
    "\n",
    "ref http://www.stat.cmu.edu/~larry/=stat705/Lecture6.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. What is Maximum Likelihood estimation (MLE) ? Can you give an example? \n",
    "MLE is a simple method of finding the unknown parameter(s) $\\theta $ of distribution/model.\n",
    " For any given observed data $x_1,x_2 · · · , x_n$, MLE estimate the value of the parameter $\\theta$  for which the likelihood function L(θ) is a maximum or in other word,  MLE estimate the value of the parameter $\\theta$  for which the likelihood of observing the data is maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. How is linear regression related to Pytorch and gradient descent? \n",
    "Linear regression is machine learning technique that can be utlized to find the linear relationship between predictor (independent variable) and predicted (dependent) variable by minimizing what's called an mean squared error (MSE). In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias as shown below.\n",
    "$$ target = w_1*input_1+ w_2*input_2+bias$$\n",
    "The learning of linear regression model is to find the suitble values for above weights and bias term. This is done by adjusting the weights many times that minimizes cost function (MSE)using an optimization technique called gradient descent.\n",
    "\n",
    "From calculus we know that the increase or decrease in loss by changing a weight element is proportional to the value of the gradient of the loss w.r.t. that element.\n",
    "\n",
    "With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases by setting parameter 'requires_grad'  to True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.Write out MSE loss for linear regression. Could we also use this loss for classification? \n",
    "Mean squared error (MSE) is the most commonly used loss function for regression. The loss is the mean of the squared differences between true and predicted values. The formula for MSE is as below.\n",
    "$$ MSE = 1/N \\sum_{n=1}^{N}(y_i - \\hat{ y_i})^2$$\n",
    "where $y_i$ is true values and $\\hat{ y_i}$ is predicted values.\n",
    "\n",
    "No, we can not use this loss function for classification because for mse to calculate we need to have continious value output like price of house, height of person but in classification we have categorical output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Write out the likelihood for linear regression. How is this related to the MSE loss for linear regression derived in the last point? Derive the relation between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.Write out the likelihood function for linear classification. What is the drawback of using MSE loss here? \n",
    "the logit model the output variable  $y_{i}$ is a Bernoulli random variable (it can take only two values, either 1 or 0)\n",
    "The likelihood function for logistic regression which is linear classification is\n",
    "\n",
    "$$j(\\theta) = 1/m \\sum_{n=1}^{m}y^{(i)}log(h_{\\theta}x^{(i)})+(1-y^{(i)}log(1-h_{\\theta}x^{(i)})$$\n",
    "\n",
    "If we try to use the cost function , i.e MSE of the linear regression in ‘Logistic Regression’ then it would be of no use as it would end up being a non-convex function with many local minimums, in which it would be very difficult to minimize the cost value and find the global minimum. In logistic regression we have the sigmoid function around, which is non-linear (i.e. not a line) as in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why?  \n",
    "Yes, gradient descent can be used to find the parameters for linear regression. Gradient descent can also be used in linear classification as the loss function is differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.What are normal equations? Is it the same as least squares? Explain. \n",
    "\n",
    "Normal equations are  technique for computing coefficients for Multivariate Linear Regression.\n",
    "This problem is also called OLS Regression, and Normal Equation is an approach of solving it\n",
    "It finds the regression coefficients analytically.\n",
    "It's an one-step learning algorithm as opposed to Gradient Descent which is iterative process of finding the regression coefficients\n",
    " This approach is an effective and a time-saving option when are working with a dataset with small features.<br>\n",
    "__Normal Equation is a follows :__\n",
    "$$ \\theta = ({X}^T{X})^{-1}.({X}^T{y}) $$\n",
    "In the above equation,<br>\n",
    "$θ$ : hypothesis parameters that define it the best.<br>\n",
    "$X$ : Input feature value of each instance.<br>\n",
    "$Y$ : Output value of each instance.<br>\n",
    "__Maths Behind the equation –__\n",
    "Given the hypothesis function <br>\n",
    "$$ h(\\theta) = \\theta_0{x_0} + \\theta_1{x_1}+...... + \\theta_n{x_n} $$\n",
    "where,<br>\n",
    "$n$ : the no. of features in the data set.<br>\n",
    "${x_0}$ : 1 (for vector multiplication)<br>\n",
    "Notice that this is dot product between θ and x values. So for the convenience to solve we can write it as :<br>\n",
    "$$ h(\\theta) = \\theta ^ T{x}$$\n",
    "The motive in Linear Regression is to minimize the cost function :<br>\n",
    "$$J(\\Theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\frac{1}{2} [h_{\\Theta}(x^{(i)}) - y^{(i)}]^{2} $$\n",
    "where,<br>\n",
    "$x_i$ : the input value of iih training example.<br>\n",
    "$m$ : no. of training instances<br>\n",
    "$n$ : no. of data-set features<br>\n",
    "$y_i$ : the expected result of ith instance<br>\n",
    "Let us representing cost function in a vector form<br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h_\\theta ({x}^0) \\\\\n",
    "h_\\theta ({x}^1) \\\\\n",
    ".......\\\\\n",
    "h_\\theta ({x}^m) \\\\\n",
    "\\end{bmatrix}\n",
    "- \\begin{bmatrix}\n",
    "({y}^0) \\\\\n",
    "({y}^1) \\\\\n",
    ".......\\\\\n",
    "({y}^m) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br><br>we have ignored 1/2m here as it will not make any difference in the working. It was used for the mathematical convenience while calculation gradient descent. But it is no more needed here.<br>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta ^ T ({x}^0) \\\\\n",
    "\\theta ^ T ({x}^1) \\\\\n",
    ".......\\\\\n",
    "\\theta ^ T ({x}^m) \\\\\n",
    "\\end{bmatrix} - y\n",
    "$$\n",
    "$\\theta_0 \\begin{pmatrix} 0   \\\\ {x_0} \\end{pmatrix}$ +\n",
    "$\\theta_1\n",
    "\\begin{pmatrix}\n",
    "0   \\\\\n",
    "{x_1}\n",
    "\\end{pmatrix}\n",
    "$\n",
    "${x}^i_j$ : value of ${j}^{ih}$ feature in ${i}^{ih}$ training example.\n",
    "This can further be reduced to  $X\\theta - y$<br>\n",
    "But each residual value is squared. We cannot simply square the above expression. As the square of a vector/matrix is not equal to the square of each of its values. So to get the squared value, multiply the vector/matrix with its transpose. So, the final equation derived is\n",
    "$$(X\\theta - y)^{T}(X\\theta - y)$$\n",
    "Therefore, the cost function is\n",
    "$$Cost = (X\\theta - y)^{T}(X\\theta - y) $$\n",
    "So, now getting the value of θ using derivative\n",
    "$$\\frac{\\partial J_{\\theta}}{\\partial {\\theta}} = \\frac{\\partial}{\\partial {\\theta}}{[(X{\\theta}- y)^T{(X{\\theta}- y)}]}$$\n",
    "$$ \\frac{\\partial J_{\\theta}}{\\partial {\\theta}} = 2X^TX\\theta - 2X^Ty$$\n",
    "$$ Cost^{'}(\\theta) = 0 $$\n",
    "$$2X^{T}X{\\theta} - 2X^Ty = 0$$\n",
    "$$2X^{T}X{\\theta} = 2X^Ty$$\n",
    "$$ (X^TX)^{-1}(X^TX){\\theta} = (X^TX)^{-1}.(X^Ty) $$\n",
    "$$\\theta = (X^TX)^{-1}.(X^Ty)$$\n",
    "So, this is the finally derived Normal Equation with θ giving the minimum cost value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Is feature scaling needed for linear regression when using gradient descent?  Why or why not? \n",
    "Yes. Feature scaling is needed for linear regression when using gradient descent. This is because real-world data can come up in different orders of magnitude. For example, human age might ranges from 0 to 100 years, while income from €10,000 to €10,000,000 (and more). Using such data with such variable range as input features for a linear regression system might slow down the gradient descent algorithm to a crawl. We can speed up gradient descent by scaling features. This is because coeffiecient of linear regression will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
